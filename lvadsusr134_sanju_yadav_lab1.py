# -*- coding: utf-8 -*-
"""LVADSUSR134_SANJU_YADAV_LAB1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xfkYMUKqCC2fY7f_a2zqRk1EOnI1wXth
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/winequality-red.csv')
df.head()

df.info()

#checking for missing values
df.isnull().sum()

df.fillna(df.mean(),inplace=True)

df.isnull().sum()

plt.figure(figsize=(10, 6))
sns.boxplot(data=df)
plt.title('Boxplot of Wine Dataset')
plt.xticks(rotation=45)
plt.show()

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)
df[outliers] = np.nan

df.fillna(df.mean(), inplace=True)

plt.figure(figsize=(10, 6))
sns.boxplot(data=df)
plt.title('wine dataset')
plt.xticks(rotation=45)
plt.show()

df.describe()
df.shape

def categorize_quality(quality):
    if quality >= 3 and quality < 7:
        return "Bad"
    elif quality >= 7 and quality <= 8:
        return "Good"

df['quality_category'] = df['quality'].apply(categorize_quality)

df.head(5)

df.info()

df.head()

df.drop(columns=["quality_category_Good"],inplace = True)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['quality_category_numeric'] = le.fit_transform(df['quality_category'])

df.head()

correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

sns.pairplot(df, vars=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'], hue='quality')
plt.show()

from sklearn.ensemble import RandomForestClassifier

X = df.drop(columns=['quality_category',	'quality_category_numeric','quality'])  # Features
y = df['quality_category_numeric']  # Target variable

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X, y)

feature_importances = pd.Series(rf.feature_importances_, index=X.columns)
feature_importances.nlargest(10).plot(kind='barh')
plt.title('Top 10')
plt.show()

top_features = feature_importances.nlargest(5).index.tolist()
selected_df = df[top_features + ['quality_category_numeric']]

selected_df.head()

selected_df.drop_duplicates(inplace=True) #removing the duplicates

selected_df.shape

from sklearn.model_selection import train_test_split


X= selected_df.drop(columns=['quality_category_numeric'])  # Features
y= selected_df['quality_category_numeric']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)
y_pred = rf_classifier.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report


accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

precision = precision_score(y_test, y_pred, average='weighted')
print("Precision:", precision)

recall = recall_score(y_test, y_pred, average='weighted')
print("Recall:", recall)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

