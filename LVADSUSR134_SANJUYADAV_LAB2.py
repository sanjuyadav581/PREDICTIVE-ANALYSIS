# -*- coding: utf-8 -*-
"""SANJU_YADAV_LAB1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tVcM6-56NwzEX5pi6K6P4YSNfcMiZhVO
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from scipy import stats
import numpy as np

data=pd.read_csv('/content/expenses.csv')
data

data.describe()

data.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

# Identifying outliers using boxplots
fig, axs = plt.subplots(3, figsize=(10, 15))

sns.boxplot(data['age'], ax=axs[0]).set_title('Age Distribution')
sns.boxplot(data['bmi'], ax=axs[1]).set_title('BMI Distribution')
sns.boxplot(data['charges'], ax=axs[2]).set_title('Charges Distribution')

plt.tight_layout()
plt.show()

z_scores = stats.zscore(data['charges'])
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3) # Filtering out outliers
data_cleaned = data[filtered_entries]
data_cleaned

#encoding
encoder = OneHotEncoder(sparse=False, drop='first')
categorical_columns = ['sex', 'smoker', 'region']
encoded_data = encoder.fit_transform(data_cleaned[categorical_columns])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))

#combing the data with above encoded one
data_final = data_cleaned.drop(categorical_columns, axis=1).reset_index(drop=True)
data_final = pd.concat([data_final, encoded_df], axis=1)
data_final

# Divide data into train and test
X = data_final.drop('charges', axis=1)
y = data_final['charges']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape

y_train.shape

model = LinearRegression()
model.fit(X_train, y_train)

# Model Evaluation
predictions = model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, predictions)

print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R^2 Score: {r2}')

# Mean Squared Error (MSE) and Gradient Descent:
# MSE is also called cost function in regression problems.
# But GD is like an optimization algo to minimise the above MSE.

# Learning Rate:
# It is a hyperparameter that controls weight wrt loss gradient.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
print("Dataset information:")
print(data.info())
print("\nSummary statistics:")
print(data.describe())
print("\nMissing values:")
print(data.isnull().sum())
print("\nDuplicate rows:")
print(data.duplicated().sum())
sns.pairplot(data)
plt.show()
categorical_vars = ['sex', 'smoker', 'region']
for var in categorical_vars:
    sns.countplot(x=var, data=data)
    plt.title(f'Distribution of {var}')
    plt.show()
sns.pairplot(data, x_vars=['age', 'bmi', 'children'], y_vars='charges', kind='scatter')
plt.show()
for var in categorical_vars:
    sns.boxplot(x=var, y='charges', data=data)
    plt.title(f'Relationship between {var} and charges')
    plt.show()

"""Importance of learning rate Convergence Speed: The learning rate influences how quickly or slowly the optimization algorithm converges to the optimal solution. A larger learning rate leads to faster convergence but increases the risk of overshooting the minimum or oscillating around it. Conversely, a smaller learning rate may converge slowly but provides more stable convergence.

Role of Derivatives and Partial Derivatives: Gradient Descent: Derivatives and partial derivatives are essential in optimization algorithms like gradient descent, which is used to minimize the loss function during model training. The derivative of the loss function with respect to each parameter (weights and biases) indicates the direction of steepest ascent or descent in the parameter space.
"""



